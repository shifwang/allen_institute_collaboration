{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from staNMF import staNMF\n",
    "from staNMF import spams_nmf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = spams_nmf(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF(10).fit(np.random.uniform(0,1,(100,20))).n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29887635, 0.56874553, 0.17862432, 0.5325737 , 0.64669147,\n",
       "        0.14206538, 0.58138896, 0.47918994, 0.38641911, 0.44046495],\n",
       "       [0.40475733, 0.44225404, 0.03012328, 0.77600531, 0.55095838,\n",
       "        0.3810734 , 0.52926578, 0.9568769 , 0.17582131, 0.11830284],\n",
       "       [0.86294567, 0.07486688, 0.82509486, 0.83613181, 0.07539491,\n",
       "        0.01140079, 0.04842057, 0.35712271, 0.66569338, 0.01138961],\n",
       "       [0.10791777, 0.9010131 , 0.79487876, 0.81146098, 0.64027806,\n",
       "        0.62477951, 0.14550751, 0.5702159 , 0.0651125 , 0.31341257],\n",
       "       [0.21706577, 0.52695039, 0.42710838, 0.98263043, 0.14045149,\n",
       "        0.7484116 , 0.78884039, 0.73039662, 0.84561834, 0.55619038],\n",
       "       [0.05686271, 0.88672457, 0.31441695, 0.56039187, 0.2540144 ,\n",
       "        0.9349621 , 0.03424494, 0.68654496, 0.43893494, 0.97772095],\n",
       "       [0.47738415, 0.23895263, 0.93235066, 0.13671513, 0.17192781,\n",
       "        0.50305449, 0.05585289, 0.29766707, 0.92771357, 0.38659638],\n",
       "       [0.46212547, 0.57862899, 0.1139206 , 0.36841365, 0.36074842,\n",
       "        0.88001849, 0.27454204, 0.22458118, 0.81397318, 0.05820075],\n",
       "       [0.73015342, 0.03267909, 0.59998723, 0.03054111, 0.85960489,\n",
       "        0.43977952, 0.56044466, 0.81740526, 0.0890063 , 0.70148773],\n",
       "       [0.38557212, 0.01742254, 0.05096687, 0.43507174, 0.97684387,\n",
       "        0.09378331, 0.5515195 , 0.64310314, 0.44025463, 0.78887382]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.uniform(0,1,(10,10))#(range(10),10,replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition.nmf import _initialize_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.52083907],\n",
       "        [0.59310116],\n",
       "        [0.75356297],\n",
       "        [0.43430839],\n",
       "        [0.11206552],\n",
       "        [0.14575903],\n",
       "        [0.49271947],\n",
       "        [0.21067894],\n",
       "        [0.49967846],\n",
       "        [0.50551794]]),\n",
       " array([[0.09155186, 0.35798257, 0.26684915, 0.25036661, 0.18874894,\n",
       "         0.05854666, 0.37476728, 0.32963969, 0.12990297, 0.01319484]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(None)\n",
    "_initialize_nmf(np.eye(10), 1, 'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def check_random_state(seed):\\n    \"\"\"Turn seed into a np.random.RandomState instance\\n\\n    Parameters\\n    ----------\\n    seed : None | int | instance of RandomState\\n        If seed is None, return the RandomState singleton used by np.random.\\n        If seed is an int, return a new RandomState instance seeded with seed.\\n        If seed is already a RandomState instance, return it.\\n        Otherwise raise ValueError.\\n    \"\"\"\\n    if seed is None or seed is np.random:\\n        return np.random.mtrand._rand\\n    if isinstance(seed, (numbers.Integral, np.integer)):\\n        return np.random.RandomState(seed)\\n    if isinstance(seed, np.random.RandomState):\\n        return seed\\n    raise ValueError(\\'%r cannot be used to seed a numpy.random.RandomState\\'\\n                     \\' instance\\' % seed)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(check_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.decomposition.nmf.NMF"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMF().__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.base import TransformerMixin\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-715a051752a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minspect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNMF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "inspect(NMF.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class NMF(BaseEstimator, TransformerMixin):\\n    r\"\"\"Non-Negative Matrix Factorization (NMF)\\n\\n    Find two non-negative matrices (W, H) whose product approximates the non-\\n    negative matrix X. This factorization can be used for example for\\n    dimensionality reduction, source separation or topic extraction.\\n\\n    The objective function is::\\n\\n        0.5 * ||X - WH||_Fro^2\\n        + alpha * l1_ratio * ||vec(W)||_1\\n        + alpha * l1_ratio * ||vec(H)||_1\\n        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\\n        + 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2\\n\\n    Where::\\n\\n        ||A||_Fro^2 = \\\\sum_{i,j} A_{ij}^2 (Frobenius norm)\\n        ||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\\n\\n    For multiplicative-update (\\'mu\\') solver, the Frobenius norm\\n    (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss,\\n    by changing the beta_loss parameter.\\n\\n    The objective function is minimized with an alternating minimization of W\\n    and H.\\n\\n    Read more in the :ref:`User Guide <NMF>`.\\n\\n    Parameters\\n    ----------\\n    n_components : int or None\\n        Number of components, if n_components is not set all features\\n        are kept.\\n\\n    init : None | \\'random\\' | \\'nndsvd\\' |  \\'nndsvda\\' | \\'nndsvdar\\' | \\'custom\\'\\n        Method used to initialize the procedure.\\n        Default: None.\\n        Valid options:\\n\\n        - None: \\'nndsvd\\' if n_components <= min(n_samples, n_features),\\n            otherwise random.\\n\\n        - \\'random\\': non-negative random matrices, scaled with:\\n            sqrt(X.mean() / n_components)\\n\\n        - \\'nndsvd\\': Nonnegative Double Singular Value Decomposition (NNDSVD)\\n            initialization (better for sparseness)\\n\\n        - \\'nndsvda\\': NNDSVD with zeros filled with the average of X\\n            (better when sparsity is not desired)\\n\\n        - \\'nndsvdar\\': NNDSVD with zeros filled with small random values\\n            (generally faster, less accurate alternative to NNDSVDa\\n            for when sparsity is not desired)\\n\\n        - \\'custom\\': use custom matrices W and H\\n\\n    solver : \\'cd\\' | \\'mu\\'\\n        Numerical solver to use:\\n        \\'cd\\' is a Coordinate Descent solver.\\n        \\'mu\\' is a Multiplicative Update solver.\\n\\n        .. versionadded:: 0.17\\n           Coordinate Descent solver.\\n\\n        .. versionadded:: 0.19\\n           Multiplicative Update solver.\\n\\n    beta_loss : float or string, default \\'frobenius\\'\\n        String must be in {\\'frobenius\\', \\'kullback-leibler\\', \\'itakura-saito\\'}.\\n        Beta divergence to be minimized, measuring the distance between X\\n        and the dot product WH. Note that values different from \\'frobenius\\'\\n        (or 2) and \\'kullback-leibler\\' (or 1) lead to significantly slower\\n        fits. Note that for beta_loss <= 0 (or \\'itakura-saito\\'), the input\\n        matrix X cannot contain zeros. Used only in \\'mu\\' solver.\\n\\n        .. versionadded:: 0.19\\n\\n    tol : float, default: 1e-4\\n        Tolerance of the stopping condition.\\n\\n    max_iter : integer, default: 200\\n        Maximum number of iterations before timing out.\\n\\n    random_state : int, RandomState instance or None, optional, default: None\\n        If int, random_state is the seed used by the random number generator;\\n        If RandomState instance, random_state is the random number generator;\\n        If None, the random number generator is the RandomState instance used\\n        by `np.random`.\\n\\n    alpha : double, default: 0.\\n        Constant that multiplies the regularization terms. Set it to zero to\\n        have no regularization.\\n\\n        .. versionadded:: 0.17\\n           *alpha* used in the Coordinate Descent solver.\\n\\n    l1_ratio : double, default: 0.\\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\\n        (aka Frobenius Norm).\\n        For l1_ratio = 1 it is an elementwise L1 penalty.\\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\\n\\n        .. versionadded:: 0.17\\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\\n           solver.\\n\\n    verbose : bool, default=False\\n        Whether to be verbose.\\n\\n    shuffle : boolean, default: False\\n        If true, randomize the order of coordinates in the CD solver.\\n\\n        .. versionadded:: 0.17\\n           *shuffle* parameter used in the Coordinate Descent solver.\\n\\n    Attributes\\n    ----------\\n    components_ : array, [n_components, n_features]\\n        Factorization matrix, sometimes called \\'dictionary\\'.\\n\\n    reconstruction_err_ : number\\n        Frobenius norm of the matrix difference, or beta-divergence, between\\n        the training data ``X`` and the reconstructed data ``WH`` from\\n        the fitted model.\\n\\n    n_iter_ : int\\n        Actual number of iterations.\\n\\n    Examples\\n    --------\\n    >>> import numpy as np\\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\\n    >>> from sklearn.decomposition import NMF\\n    >>> model = NMF(n_components=2, init=\\'random\\', random_state=0)\\n    >>> W = model.fit_transform(X)\\n    >>> H = model.components_\\n\\n    References\\n    ----------\\n    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for\\n    large scale nonnegative matrix and tensor factorizations.\"\\n    IEICE transactions on fundamentals of electronics, communications and\\n    computer sciences 92.3: 708-721, 2009.\\n\\n    Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix\\n    factorization with the beta-divergence. Neural Computation, 23(9).\\n    \"\"\"\\n\\n    def __init__(self, n_components=None, init=None, solver=\\'cd\\',\\n                 beta_loss=\\'frobenius\\', tol=1e-4, max_iter=200,\\n                 random_state=None, alpha=0., l1_ratio=0., verbose=0,\\n                 shuffle=False):\\n        self.n_components = n_components\\n        self.init = init\\n        self.solver = solver\\n        self.beta_loss = beta_loss\\n        self.tol = tol\\n        self.max_iter = max_iter\\n        self.random_state = random_state\\n        self.alpha = alpha\\n        self.l1_ratio = l1_ratio\\n        self.verbose = verbose\\n        self.shuffle = shuffle\\n\\n    def fit_transform(self, X, y=None, W=None, H=None):\\n        \"\"\"Learn a NMF model for the data X and returns the transformed data.\\n\\n        This is more efficient than calling fit followed by transform.\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\\n            Data matrix to be decomposed\\n\\n        y : Ignored\\n\\n        W : array-like, shape (n_samples, n_components)\\n            If init=\\'custom\\', it is used as initial guess for the solution.\\n\\n        H : array-like, shape (n_components, n_features)\\n            If init=\\'custom\\', it is used as initial guess for the solution.\\n\\n        Returns\\n        -------\\n        W : array, shape (n_samples, n_components)\\n            Transformed data.\\n        \"\"\"\\n        X = check_array(X, accept_sparse=(\\'csr\\', \\'csc\\'), dtype=float)\\n\\n        W, H, n_iter_ = non_negative_factorization(\\n            X=X, W=W, H=H, n_components=self.n_components, init=self.init,\\n            update_H=True, solver=self.solver, beta_loss=self.beta_loss,\\n            tol=self.tol, max_iter=self.max_iter, alpha=self.alpha,\\n            l1_ratio=self.l1_ratio, regularization=\\'both\\',\\n            random_state=self.random_state, verbose=self.verbose,\\n            shuffle=self.shuffle)\\n\\n        self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\\n                                                    square_root=True)\\n\\n        self.n_components_ = H.shape[0]\\n        self.components_ = H\\n        self.n_iter_ = n_iter_\\n\\n        return W\\n\\n    def fit(self, X, y=None, **params):\\n        \"\"\"Learn a NMF model for the data X.\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\\n            Data matrix to be decomposed\\n\\n        y : Ignored\\n\\n        Returns\\n        -------\\n        self\\n        \"\"\"\\n        self.fit_transform(X, **params)\\n        return self\\n\\n    def transform(self, X):\\n        \"\"\"Transform the data X according to the fitted NMF model\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\\n            Data matrix to be transformed by the model\\n\\n        Returns\\n        -------\\n        W : array, shape (n_samples, n_components)\\n            Transformed data\\n        \"\"\"\\n        check_is_fitted(self, \\'n_components_\\')\\n\\n        W, _, n_iter_ = non_negative_factorization(\\n            X=X, W=None, H=self.components_, n_components=self.n_components_,\\n            init=self.init, update_H=False, solver=self.solver,\\n            beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter,\\n            alpha=self.alpha, l1_ratio=self.l1_ratio, regularization=\\'both\\',\\n            random_state=self.random_state, verbose=self.verbose,\\n            shuffle=self.shuffle)\\n\\n        return W\\n\\n    def inverse_transform(self, W):\\n        \"\"\"Transform data back to its original space.\\n\\n        Parameters\\n        ----------\\n        W : {array-like, sparse matrix}, shape (n_samples, n_components)\\n            Transformed data matrix\\n\\n        Returns\\n        -------\\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\\n            Data matrix of original shape\\n\\n        .. versionadded:: 0.18\\n        \"\"\"\\n        check_is_fitted(self, \\'n_components_\\')\\n        return np.dot(W, self.components_)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'staNMF' has no attribute 'f_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-93657497dc9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstaNMF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36mgetargvalues\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0;34m'varargs'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'varkw'\u001b[0m \u001b[0mare\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnames\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     'locals' is the locals dictionary of the given frame.\"\"\"\n\u001b[0;32m-> 1189\u001b[0;31m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mArgInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'staNMF' has no attribute 'f_code'"
     ]
    }
   ],
   "source": [
    "inspect.getargvalues(staNMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
